

flux: "efiitg_gb"
Nbootstraps: 13 # if None, runs only one instance otherwise it spawns Nbootstraps instances

data:
  train: "/home/ir-zani1/rds/rds-ukaea-ap001/ir-zani1/qualikiz/UKAEAGroupProject/data/train_data_clipped.pkl"
  validation: "/home/ir-zani1/rds/rds-ukaea-ap001/ir-zani1/qualikiz/UKAEAGroupProject/data/valid_data_clipped.pkl"
  test: "/home/ir-zani1/rds/rds-ukaea-ap001/ir-zani1/qualikiz/UKAEAGroupProject/data/test_data_clipped.pkl"
save_paths:
  plots: "/home/ir-zani1/rds/rds-ukaea-ap001/ir-zani1/qualikiz/UKAEAGroupProject/plots/"
  outputs: "/home/ir-zani1/rds/rds-ukaea-ap001/ir-zani1/qualikiz/UKAEAGroupProject/outputs/"

retrain_classifier: True
retrain_regressor: True


# Use pre-trained model or train from scratch
pretrained:
  Classifier:
      efiitg_gb:
          save_path: "/home/ir-zani1/rds/rds-ukaea-ap001/ir-zani1/qualikiz/UKAEAGroupProject/saved/ITGclassifier.pt"
          trained: False
      efetem_gb:
          save_path: "/home/ir-zani1/rds/rds-ukaea-ap001/ir-zani1/qualikiz/UKAEAGroupProject/saved/TEMclassifier.pt"
          trained: False
      efeetg_gb:
          save_path: "/home/ir-zani1/rds/rds-ukaea-ap001/ir-zani1/qualikiz/UKAEAGroupProject/saved/ETGclassifier.pt"
          trained: False
  Regressor:
      efiitg_gb:
          save_path: "/home/ir-zani1/rds/rds-ukaea-ap001/ir-zani1/qualikiz/UKAEAGroupProject/saved/ITGregressor.pt"
          trained: False
      efetem_gb:
          save_path: "/home/ir-zani1/rds/rds-ukaea-ap001/ir-zani1/qualikiz/UKAEAGroupProject/saved/TEMclassifier.pt"
          trained: False
      efeetg_gb:
          save_path: "/home/ir-zani1/rds/rds-ukaea-ap001/ir-zani1/qualikiz/UKAEAGroupProject/saved/ETGclassifier.pt"
          trained: True

hyperparams:
    train_size: 5_000
    valid_size: 2_000
    test_size:  2_000
    candidate_size: 10_000 
    batch_size: 512
    lambda: 1
    buffer_size: 256
    model_size: 'shallow_wide'    #deep

logging_level: DEBUG
# Pipeline parameters
iterations: 20
initial_epochs: 30
MC_dropout_runs: 50
keep_prob: 0.25 
patience: 10
learning_rate: 0.001
sample_size_debug: 0.1

# if training models from scratch, the hyperparameters below are used
train_epochs: 30
train_patience: 10


